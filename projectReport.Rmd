---
title: "Human Activity Recognition with Weight Lifting"
output: html_notebook
---

```{r, echo = FALSE, results = "none"}
library(tidyverse)
library(skimr)
library(reshape2)
library(caret)
library(ggplot2)
library(ggthemes)
library(doParallel) # parallel processing on desktop
```

# Executive Summary

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this report, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

# Data

```{r, cache=TRUE}

# check if exists, then download from sources
if (!file.exists("training_data.csv")){
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training_data.csv")
  
}

if (!file.exists("training_data.csv")){
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing_data.csv")
}

#import data
training <- read_csv("training_data.csv")
testing <- read_csv("testing_data.csv")
```

We find that due to the number of unique entries in the character columns, we'll simply convert each to factor variables, except for 'cvtd_timestamp', as this is a date.

```{r}
training$classe <- as.factor(training$classe)
```

We find that there are multiple variables that have no entries at all. We will first remove these variables. 

We also find that `...1` is simply a reference number for each observation, and should not be included either.

Other columns we will not include are `user_name` and datetime variables. As this may include bias depending on simply when a participant decided to perform incorrectly.

```{r}
# obtain columns with no observations
col_na <- colnames(training)[colSums(is.na(training)) > 0]
training <- select(training, -c(all_of(col_na)))
training <- select(training, -c("...1", "new_window",
                                "num_window", "user_name",
                                "raw_timestamp_part_1",
                                "raw_timestamp_part_2",
                                "cvtd_timestamp"))
```



Because we are building a model to predict the `classe` of an observation, it would be best used with variable that provide a range of values based upon the `classe`. To investigate this, we produced a boxplot for each of the numeric columns and we'll only include those that produce values that differ somewhat for `classe`.

```{r}
# get columns where the variance of that column is above #
trainVar <- training %>% summarise_if(is.numeric, var)

varLevel <- which(trainVar >= 100, arr.ind = T)

col_selection <- names(trainVar[varLevel[,2]])

# choosing only relevant columns
training <- training[, c(col_selection, "classe")]
```

# Modelling

Now that we have trimmed the variables down to the most relevant ones, we can now choose some models and fit for each. 

With `40` variables at this stage, with various magnitudes, we'll opt to utilise trees, that is Random Forests and then stack using a Gradient Boosted tree.

To ensure reliability, and to reduce bias, we will utilise cross-fold validation using the `trainControl` function from `caret`.

Firstly, we will have to split our training data. We will do a 3-split as we want to also stack our models.

```{r}
# splitting the data
set.seed(17373)

trainIndex <- createDataPartition(training$classe,
                                  p = .8, list = FALSE, times = 1)

wTrain_ <- training[trainIndex,]
wTest <- training[-trainIndex,]

# split test further to create a validation set

valIndex <- createDataPartition(wTrain_$classe,
                                p = .7, list = FALSE, times = 1)

wTrain <- wTrain_[valIndex,]
wVal <- wTrain_[-valIndex,]

print(c(dim(wTrain), dim(wVal), dim(wTest)))
```
This split gives us `r dim(wTrain)[1]` rows for Training, `r dim(wVal)[1]` for validating our stacked models, and `r dim(wTest)[1]` for testing.

```{r}
# 5-fold cross-validaiton
set.seed(37)

ctrl <- trainControl(method = "cv", number = 3)
RFgrid <- expand.grid(mtry = c(2:4))
RBFgrid <- expand.grid(C = c(0.01, 0.03, 0.1, 0.3, 1.0, 3.0),
                       sigma = c(0.001, 0.003, 0.01, 0.03, 0.1, 0.3))
SVMgrid <- expand.grid(C = c(0.01, 0.03, 0.1, 0.3, 1.0, 3.0))
```

```{r}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wRF <- train(classe ~ ., method = "rf", tfControl = ctrl,
             preProcess = c("center", "scale"), tuneGrid = RFgrid,
             data = wTrain)

stopCluster(cl) # stop parallel processing

pred_wRF <- predict(wRF, newdata = wTest)

confusionMatrix(pred_wRF, wTest$classe)
```
```{r}
png(filename = "tuning_plot_rf.png", width = 600, height = 400)
plot(wRF, main = "Tuning Parameters for Random Forest")
dev.off()
```


```{r}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wRBF <- train(classe ~ ., method = "svmRadial", tfControl = ctrl,
              preProcess = c("center", "scale"), tuneGrid = RBFgrid,
              tuneLength = 10, data = wTrain)

stopCluster(cl) # stop parallel processing

pred_wRBF <- predict(wRBF, newdata = wTest)

confusionMatrix(pred_wRBF, wTest$classe)
```
```{r}
png(filename = "tuning_plot_rbf.png", width = 600, height = 400)
plot(wRBF, main = "Tuning Parameters for Radial Basis Function")
dev.off()
```


```{r}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wSVM <- train(classe ~ ., method = "svmLinear",
              tfControl = ctrl, 
              preProcess = c("center", "scale"),
              tuneGrid = SVMgrid,
              tuneLength = 10, data = wTrain)

stopCluster(cl) # stop parallel processing

pred_wSVM <- predict(wSVM, newdata = wTest)

confusionMatrix(pred_wSVM, wTest$classe)
```

```{r}
png(filename = "tuning_plot_svm.png", width = 600, height = 400)
plot(wSVM, main = "Tuning Parameters for Support Vector Machine")
dev.off()
```

```{r}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)


# a boosted classification tree for stacking
wGBM <- train(classe ~ ., method = "gbm",
              preProcess = c("center", "scale"),
              data = wTrain)

stopCluster(cl) # stop parallel processing

pred_wGBM <- predict(wGBM, newdata = wTest)
confusionMatrix(pred_wGBM, wTest$classe)
```


```{r}
png(filename = "tuning_plot_gbm.png", width = 600, height = 400)
plot(wGBM, main = "Tuning Parameters for Gradient Boosted Classifier")
dev.off()
```

From the models, we can see that the best parameters are
- Random Forest: predictors = 4
- Radial Basis Function: Cost = 3.0, sigma = 0.03
- Support Vector Machine: Cost = 3.0
- Gradient Boosted Classifier: Max Iter = 150, Max Depth = 3

Well combine the best three models with a random forest to create a blended model

```{r}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

in_Train <- data.frame(classe = wTest$classe,
                       RF = pred_wRF,
                       RBF = pred_wRBF,
                       GBM = pred_wGBM)

wCOMB <- train(classe ~ ., method = "rf",
             tfControl = ctrl,
             preProcess = c("center", "scale"),
             tuneGrid = RFgrid,
             data = in_Train)

stopCluster(cl) # stop parallel processing

pred_wCOMB <- predict(wCOMB, newdata = in_Train)
confusionMatrix(pred_wCOMB, wTest$classe)
```

```{r}
png(filename = "tuning_plot_comb.png", width = 600, height = 400)
plot(wCOMB, main = "Tuning Parameters for blended model")
dev.off()
```

```{r}
val_wRF <- predict(wRF, newdata = wVal)
confusionMatrix(val_wRF, wVal$classe)
```

```{r}
val_wRBF <- predict(wRBF, newdata = wVal)
confusionMatrix(val_wRBF, wVal$classe)
```

```{r}
val_wGBM <- predict(wGBM, newdata = wVal)
confusionMatrix(val_wGBM, wVal$classe)
```

```{r}
in_Val <- data.frame(classe = wVal$classe,
                       RF = val_wRF,
                       RBF = val_wRBF,
                       GBM = val_wGBM)

val_wCOMB <- predict(wCOMB, newdata = in_Val)
confusionMatrix(val_wCOMB, wVal$classe)
```

