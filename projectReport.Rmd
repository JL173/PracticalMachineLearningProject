---
title: "Human Activity Recognition with Weight Lifting"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r, echo = FALSE, results = "none"}
library(tidyverse)
library(skimr)
library(reshape2)
library(caret)
library(ggplot2)
library(ggthemes)
library(doParallel) # parallel processing on desktop
```

# Executive Summary

We build a blended machine learning model using Random Forest on three models, Random Forest, Radial Basis Function, and Gradient Boosted Classifier, to model the activity of 6 participants performing an exercise correctly and incorrectly 4 ways, giving 5 classes of activity. The blended model has a Kappa statistic of 0.9915, indicating extremely high accuracy on the classification of these activities.

# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this report, we will use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

# Data

```{r, cache=TRUE, results = 'none'}

# check if exists, then download from sources
if (!file.exists("training_data.csv")){
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "training_data.csv")
  
}

if (!file.exists("training_data.csv")){
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "testing_data.csv")
}

#import data
training <- read_csv("training_data.csv")
testing <- read_csv("testing_data.csv")
```

We find that due to the number of unique entries in the character columns, we'll simply convert each to factor variables, except for 'cvtd_timestamp', as this is a date.

```{r}
training$classe <- as.factor(training$classe)
```

We find that there are multiple variables that have no entries at all. We will first remove these variables. 

We also find that `...1` is simply a reference number for each observation, and should not be included either.

Other columns we will not include are `user_name` and datetime variables. As this may include bias depending on simply who and when a participant decided to perform incorrectly.

```{r}
# obtain columns with no observations
col_na <- colnames(training)[colSums(is.na(training)) > 0]
training <- select(training, -c(all_of(col_na)))
training <- select(training, -c("...1", "new_window",
                                "num_window", "user_name",
                                "raw_timestamp_part_1",
                                "raw_timestamp_part_2",
                                "cvtd_timestamp"))
```



Because we are building a model to predict the `classe` of an observation, it would be best used with variable that provide a range of values based upon the `classe`. To investigate this, we produced a boxplot for each of the numeric columns and we'll only include those that produce values that differ somewhat for `classe`.

```{r}
# get columns where the variance of that column is above #
trainVar <- training %>% summarise_if(is.numeric, var)

varLevel <- which(trainVar >= 100, arr.ind = T)

col_selection <- names(trainVar[varLevel[,2]])

# choosing only relevant columns
training <- training[, c(col_selection, "classe")]
```

```{r, fig.height = 16, fig.width = 8}

plot1 <- function(df){

  df_long <- melt(df, id.vars = "classe")
  
  df_long$classe <- factor(df_long$classe,
                           levels = c("A", "B", "C", "D", "E"))
  
  ggplot(data = df_long, aes(x = classe,
                                 y = value,
                                 fill = classe)) +
    ylim(-1000, 1000)+
    geom_boxplot() + 
    facet_wrap(. ~ variable, nrow = 10)
}
plot1(training)
```

# Modelling

Now that we have trimmed the variables down to the most relevant ones, we can now choose some models and fit for each. 

With `40` variables at this stage, with various magnitudes, we'll opt to utilise the following models to minimise bias in our combined model.

- Random Forests
- Radial Basis Functions
- Support Vector Machine
- Gradient Boosted Classifier

To ensure reliability, and to reduce bias, we will utilise cross-fold validation using the `trainControl` function from `caret`.

Firstly, we will have to split our training data. We will do a 3-split as we want to also stack our models.

*Note: These models have been stored in "ProjectModels.RData" to save computing efforts.*

```{r, cache=TRUE}
if (!file.exists("ProjectModels.RData")){
    download.file("https://github.com/JL173/PracticalMachineLearningProject/raw/main/ProjectModels.RData",
                  "ProjectModels.RData")
    }
load("ProjectModels.RData")

# wRF = Random Forest Model
# wRBF = Radial Basis Function Model
# wSVM = Support Vector Machine Model
# wCOMB = combined model using Random Forest on other model predictions
```

```{r, cache=TRUE}
# splitting the data
set.seed(17373)

trainIndex <- createDataPartition(training$classe,
                                  p = .8, list = FALSE, times = 1)

wTrain_ <- training[trainIndex,]
wTest <- training[-trainIndex,]

# split test further to create a validation set

valIndex <- createDataPartition(wTrain_$classe,
                                p = .7, list = FALSE, times = 1)

wTrain <- wTrain_[valIndex,]
wVal <- wTrain_[-valIndex,]

print(c(dim(wTrain), dim(wVal), dim(wTest)))
```

This split gives us `r dim(wTrain)[1]` rows for Training, `r dim(wVal)[1]` for validating our stacked models, and `r dim(wTest)[1]` for testing.

```{r, cache=TRUE}
# 5-fold cross-validaiton
set.seed(37)

ctrl <- trainControl(method = "cv", number = 3)
RFgrid <- expand.grid(mtry = c(2:4))
RBFgrid <- expand.grid(C = c(0.01, 0.03, 0.1, 0.3, 1.0, 3.0),
                       sigma = c(0.001, 0.003, 0.01, 0.03, 0.1, 0.3))
SVMgrid <- expand.grid(C = c(0.01, 0.03, 0.1, 0.3, 1.0, 3.0))
```

```{r, cache=TRUE, eval=FALSE}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wRF <- train(classe ~ ., method = "rf", tfControl = ctrl,
             preProcess = c("center", "scale"), tuneGrid = RFgrid,
             data = wTrain)

stopCluster(cl) # stop parallel processing
```

```{r}
pred_wRF <- predict(wRF, newdata = wTest)

confusionMatrix(pred_wRF, wTest$classe)
```

```{r}
#png(filename = "tuning_plot_rf.png", width = 600, height = 400)
plot(wRF, main = "Tuning Parameters for Random Forest")
#dev.off()
```


```{r, cache=TRUE, eval=FALSE}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wRBF <- train(classe ~ ., method = "svmRadial", tfControl = ctrl,
              preProcess = c("center", "scale"), tuneGrid = RBFgrid,
              tuneLength = 10, data = wTrain)

stopCluster(cl) # stop parallel processing
```

```{r}
pred_wRBF <- predict(wRBF, newdata = wTest)

confusionMatrix(pred_wRBF, wTest$classe)
```

```{r}
#png(filename = "tuning_plot_rbf.png", width = 600, height = 400)
plot(wRBF, main = "Tuning Parameters for Radial Basis Function")
#dev.off()
```


```{r, cache=TRUE, eval=FALSE}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

wSVM <- train(classe ~ ., method = "svmLinear",
              tfControl = ctrl, 
              preProcess = c("center", "scale"),
              tuneGrid = SVMgrid,
              tuneLength = 10, data = wTrain)

stopCluster(cl) # stop parallel processing
```

```{r}
pred_wSVM <- predict(wSVM, newdata = wTest)

confusionMatrix(pred_wSVM, wTest$classe)
```


```{r}
#png(filename = "tuning_plot_svm.png", width = 600, height = 400)
plot(wSVM, main = "Tuning Parameters for Support Vector Machine")
#dev.off()
```

```{r, cache=TRUE, eval=FALSE}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)


# a boosted classification tree for stacking
wGBM <- train(classe ~ ., method = "gbm",
              preProcess = c("center", "scale"),
              data = wTrain)

stopCluster(cl) # stop parallel processing
```

```{r}
pred_wGBM <- predict(wGBM, newdata = wTest)
confusionMatrix(pred_wGBM, wTest$classe)
```

```{r}
#png(filename = "tuning_plot_gbm.png", width = 600, height = 400)
plot(wGBM, main = "Tuning Parameters for Gradient Boosted Classifier")
#dev.off()
```

From the models, we can see that the best parameters are

- Random Forest: predictors = 4
- Radial Basis Function: Cost = 3.0, sigma = 0.03
- Support Vector Machine: Cost = 3.0
- Gradient Boosted Classifier: Max Iter = 150, Max Depth = 3

Ranking the four models on their performance on the test data, by their Kappa statistic, we have

- Random Forest: Kappa = 0.9887
- Radial Basis Function: Kappa = 0.9677
- Gradient Boosted Classifier: Kappa = 0.9548
- Support Vector Machine: Kappa = 0.7323

Well combine the best three models with a random forest to create a blended model. We will train the Combined model with each of the other models' predictions on the test set. We will then use the Validation set to check the performance of the Combined model, and validate it with the other models.

```{r, cache=TRUE, eval=FALSE}
cl <- makePSOCKcluster(5) # init parallel processing
registerDoParallel(cl)

in_Train <- data.frame(classe = wTest$classe,
                       RF = pred_wRF,
                       RBF = pred_wRBF,
                       GBM = pred_wGBM)

wCOMB <- train(classe ~ ., method = "rf",
             tfControl = ctrl,
             preProcess = c("center", "scale"),
             tuneGrid = RFgrid,
             data = in_Train)

stopCluster(cl) # stop parallel processing
```

```{r}
#png(filename = "tuning_plot_comb.png", width = 600, height = 400)
plot(wCOMB, main = "Tuning Parameters for blended model")
#dev.off()
```

```{r}
val_wRF <- predict(wRF, newdata = wVal)
confusionMatrix(val_wRF, wVal$classe)
```

This Random Forest model has given us a high accuracy, `0.9913`, on the validation set, with only `41` mis-classified observations. The Kappa statistic is `0.989`, which is fair to say that there is an extremely strong level of agreement in the classification of observations.

```{r}
val_wRBF <- predict(wRBF, newdata = wVal)
confusionMatrix(val_wRBF, wVal$classe)
```

This Radial Basis Function model has given us a high accuracy, `0.9764`, on the validation set, with only `111` mis-classified observations. The Kappa statistic is `0.9702`, which is fair to say that there is a very strong level of agreement in the classification of observations.

```{r}
val_wGBM <- predict(wGBM, newdata = wVal)
confusionMatrix(val_wGBM, wVal$classe)
```

This Gradient Boosted model has given us a high accuracy, `0.9609`, on the validation set, with only `184` mis-classified observations. The Kappa statistic is `0.9505`, which is fair to say that there is a very strong level of agreement in the classification of observations.

```{r}
in_Val <- data.frame(classe = wVal$classe,
                       RF = val_wRF,
                       RBF = val_wRBF,
                       GBM = val_wGBM)

val_wCOMB <- predict(wCOMB, newdata = in_Val)
confusionMatrix(val_wCOMB, wVal$classe)
```

This Combined model has given us a high accuracy, `0.9915`, on the validation set, with only `40` mis-classified observations. The Kappa statistic is `0.9892`, which is fair to say that there is an extremely strong level of agreement in the classification of observations.

Ultimately, the Combined model makes only an improvement of *one less error* than our strongest individual model, the Random Forest model. Thus, the added complexity of including this, on top of the other two models, frankly, is not worth the effort for scaling up and training on other datasets, when the Random Forest model already has near matched performance.

Still, we will use this combined model to make the predictions of the 20 test observations.

```{r, include = FALSE}
clean_dataframe <- function(training, col_na, col_selection){
  
  training <- select(training, -c(all_of(col_na)))
  training <- select(training, -c("...1", "new_window",
                                  "num_window", "user_name",
                                  "raw_timestamp_part_1",
                                  "raw_timestamp_part_2",
                                  "cvtd_timestamp"))
  training <- training[, c(col_selection)]
}
```

```{r}
# clean_dataframe is a helper function that repeats
# the same steps as for cleaning training data
test_sample <- clean_dataframe(testing, col_na, col_selection)

test_wRF <- predict(wRF, newdata = test_sample)
test_wRBF <- predict(wRBF, newdata = test_sample)
test_wGBM <- predict(wGBM, newdata = test_sample)

in_test <- data.frame(RF = test_wRF,
                      RBF = test_wRBF,
                      GBM = test_wGBM)

#predict(wCOMB, newdata = in_test)

# all 20 were predicted correctly
```